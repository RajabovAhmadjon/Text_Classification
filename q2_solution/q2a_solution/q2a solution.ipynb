{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2α: De-Duplication with Locality Sensitive Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "import re\n",
    "import time\n",
    "from time import perf_counter\n",
    "\n",
    "from datasketch import MinHash, MinHashLSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>How many people are going towards using phones...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>What audio format should I use for getting aud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>What is the corporate culture like at Edwards ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>What is the best barbecue in Kansas City?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>\"Can I combine the output of two bolts to one ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531985</th>\n",
       "      <td>531985</td>\n",
       "      <td>Why is SEO important?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531986</th>\n",
       "      <td>531986</td>\n",
       "      <td>Who is the best employer for Robotic Process a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531987</th>\n",
       "      <td>531987</td>\n",
       "      <td>Is it possible to cure the holes caused by pim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531988</th>\n",
       "      <td>531988</td>\n",
       "      <td>How many employees does Infosys have?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531989</th>\n",
       "      <td>531989</td>\n",
       "      <td>What is the maximum size for an attachment in ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>531990 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id                                            Content\n",
       "0            0  How many people are going towards using phones...\n",
       "1            1  What audio format should I use for getting aud...\n",
       "2            2  What is the corporate culture like at Edwards ...\n",
       "3            3        What is the best barbecue in Kansas City?\\n\n",
       "4            4  \"Can I combine the output of two bolts to one ...\n",
       "...        ...                                                ...\n",
       "531985  531985                            Why is SEO important?\\n\n",
       "531986  531986  Who is the best employer for Robotic Process a...\n",
       "531987  531987  Is it possible to cure the holes caused by pim...\n",
       "531988  531988            How many employees does Infosys have?\\n\n",
       "531989  531989  What is the maximum size for an attachment in ...\n",
       "\n",
       "[531990 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = pd.read_csv('corpusTrain.csv')\n",
    "test_dataset = pd.read_csv('corpusTest.csv')\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.dropna(inplace = True)\n",
    "test_dataset.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Document Term Matrix\n",
    "count_vec = CountVectorizer()\n",
    "count_vec = count_vec.fit(train_dataset['Content'])\n",
    "\n",
    "sparse_matrix_train = count_vec.transform(train_dataset['Content'])\n",
    "sparse_matrix_test = count_vec.transform(test_dataset['Content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exact Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_build_cos = time.time()\n",
    "\n",
    "similar_items = []\n",
    "counter = 0\n",
    "col = ['Train Id', 'Test Id', 'Similarity']\n",
    "similar_items_set= DataFrame(columns = col)\n",
    "\n",
    "X_tfidf = []\n",
    "tfidf = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    ngram_range=(1, 3),\n",
    "    min_df=0,\n",
    "    stop_words='english')\n",
    "X_tfidf = tfidf.fit_transform(united['Content'])\n",
    "\n",
    "\n",
    "def get_similarity_items(X_tfidf, item_id, topn=3):\n",
    "    \"\"\"\n",
    "    Get the top similar items for a given item id.\n",
    "    The similarity measure here is based on cosine distance.\n",
    "    \"\"\"\n",
    "    query = X_tfidf[item_id]\n",
    "    scores = X_tfidf.dot(query.T).toarray().ravel()\n",
    "    best = np.argpartition(scores, -topn)[-topn:]\n",
    "    return sorted(zip(best, scores[best]), key=lambda x: -x[1])\n",
    "\n",
    "end_build_cos = time.time()\n",
    "build_time_cos = end_build_cos - start_build_cos\n",
    "start_query_cos = time.time()\n",
    "\n",
    "for i in range(X_tfidf.shape[0]-test.shape[0]):\n",
    "    similar_item = get_similarity_items(X_tfidf, item_id=i)\n",
    "    if similar_item[1][1] >= 0.8:\n",
    "        if similar_item[1][0]>= train.shape[0]: \n",
    "            similar_items_set.at[counter, 'Train Id'] = i\n",
    "            similar_items_set.at[counter, 'Test Id'] = similar_item[1][0]\n",
    "            similar_items_set.at[counter, 'Similarity'] = similar_item[1][1]\n",
    "            \n",
    "            if similar_item[2][1] >=0.8:\n",
    "                similar_items_set.at[counter, 'Train Id'] = i\n",
    "                similar_items_set.at[counter, 'Test Id'] = similar_item[2][0]\n",
    "                similar_items_set.at[counter, 'Similarity'] = similar_item[2][1]\n",
    "            \n",
    "            counter = counter + 1\n",
    "\n",
    "end_query_cos = time.time()\n",
    "query_time_cos = end_query_cos - start_query_cos\n",
    "total_time_cos = build_time_cos + query_time_cos\n",
    "print(similar_items_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exact Jaccard Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_build_jac = time.time()\n",
    "\n",
    "def get_shingles(text, char_ngram = 5):\n",
    "    return set(text[head:head +char_ngram] for head in range(0, len(text) - char_ngram))\n",
    "\n",
    "def jaccard(set_a, set_b):\n",
    "    intersection = set_a & set_b\n",
    "    union = set_a | set_b\n",
    "    return len(intersection) / len(union)\n",
    "\n",
    "shingles = []\n",
    "\n",
    "for i_line in united['Content'].index:\n",
    "    shingles.append(get_shingles(tuple(united['Content'][i_line])))\n",
    "duplicates = []\n",
    "\n",
    "end_build_jac = time.time()\n",
    "build_time_jac = end_build_jac - start_build_jac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementation of Jaccard Similarity on the 1st 1000 samples of the corpus\n",
    "\n",
    "\n",
    "start_query_jac = time.time()\n",
    "\n",
    "for i_doc in range(len(shingles)):\n",
    "    for j_doc in range(i_doc +1, len(shingles)):\n",
    "        jaccard_similarity = jaccard(shingles[i_doc], shingles[j_doc])\n",
    "        is_duplicate = jaccard_similarity >= 0.8\n",
    "        if is_duplicate:\n",
    "            duplicates.append((i_doc, j_doc, jaccard_similarity))\n",
    "\n",
    "with pd.option_context('display.precision', 2):\n",
    "    display(pd.DataFrame(duplicates, columns=['Document ID', 'Document ID', 'Jaccard Similarity']).head(n=10))\n",
    "\n",
    "end_query_jac = time.time()        \n",
    "query_time_jac = end_query_jac - start_query_jac\n",
    "\n",
    "total_time_jac = build_time_jac + query_time_jac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSH Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_build_lsh_cos = time.time()\n",
    "def generate_random_vectors(dim, n_vectors):\n",
    "    \"\"\"\n",
    "    generate random projection vectors\n",
    "    the dims comes first in the matrix's shape,\n",
    "    so we can use it for matrix multiplication.\n",
    "    \"\"\"\n",
    "    return np.random.randn(dim, n_vectors)\n",
    "def train_lsh(X_tfidf, n_vectors, seed=None):    \n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    dim = X_tfidf.shape[1]\n",
    "    random_vectors = generate_random_vectors(dim, n_vectors)  \n",
    "\n",
    "    # partition data points into bins,\n",
    "    # and encode bin index bits into integers\n",
    "    bin_indices_bits = X_tfidf.dot(random_vectors) >= 0\n",
    "    powers_of_two = 1 << np.arange(n_vectors - 1, -1, step=-1)\n",
    "    bin_indices = bin_indices_bits.dot(powers_of_two)\n",
    "\n",
    "    # update `table` so that `table[i]` is the list of document ids with bin index equal to i\n",
    "    table = defaultdict(list)\n",
    "    for idx, bin_index in enumerate(bin_indices):\n",
    "        table[bin_index].append(idx)\n",
    "    \n",
    "    # note that we're storing the bin_indices here\n",
    "    # so we can do some ad-hoc checking with it,\n",
    "    # this isn't actually required\n",
    "    model = {'table': table,\n",
    "             'random_vectors': random_vectors,\n",
    "             'bin_indices': bin_indices,\n",
    "             'bin_indices_bits': bin_indices_bits}\n",
    "    return model\n",
    "\n",
    "\n",
    "# train the model\n",
    "n_vectors = 16\n",
    "model = train_lsh(X_tfidf, n_vectors, seed=143)\n",
    "\n",
    "def search_nearby_bins(query_bin_bits, table, search_radius=3, candidate_set=None):\n",
    "    \"\"\"\n",
    "    For a given query vector and trained LSH model's table\n",
    "    return all candidate neighbors with the specified search radius.\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    model = train_lsh(X_tfidf, n_vectors=16, seed=143)\n",
    "    query = model['bin_index_bits'][0]  # vector for the first document\n",
    "    candidates = search_nearby_bins(query, model['table'])\n",
    "    \"\"\"\n",
    "    if candidate_set is None:\n",
    "        candidate_set = set()\n",
    "\n",
    "    n_vectors = query_bin_bits.shape[0]\n",
    "    powers_of_two = 1 << np.arange(n_vectors - 1, -1, step=-1)\n",
    "\n",
    "    for different_bits in combinations(range(n_vectors), search_radius):\n",
    "        # flip the bits (n_1, n_2, ..., n_r) of the query bin to produce a new bit vector\n",
    "        index = list(different_bits)\n",
    "        alternate_bits = query_bin_bits.copy()\n",
    "        alternate_bits[index] = np.logical_not(alternate_bits[index])\n",
    "\n",
    "        # convert the new bit vector to an integer index\n",
    "        nearby_bin = alternate_bits.dot(powers_of_two)\n",
    "\n",
    "        # fetch the list of documents belonging to\n",
    "        # the bin indexed by the new bit vector,\n",
    "        # then add those documents to candidate_set;\n",
    "        # make sure that the bin exists in the table\n",
    "        if nearby_bin in table:\n",
    "            candidate_set.update(table[nearby_bin])\n",
    "\n",
    "    return candidate_set\n",
    "\n",
    "def get_nearest_neighbors(X_tfidf, query_vector, model, max_search_radius=3):\n",
    "    table = model['table']\n",
    "    random_vectors = model['random_vectors']\n",
    "\n",
    "    # compute bin index for the query vector, in bit representation.\n",
    "    bin_index_bits = np.ravel(query_vector.dot(random_vectors) >= 0)\n",
    "\n",
    "    # search nearby bins and collect candidates\n",
    "    candidate_set = set()\n",
    "    for search_radius in range(max_search_radius + 1):\n",
    "        candidate_set = search_nearby_bins(bin_index_bits, table, search_radius, candidate_set)\n",
    "\n",
    "    # sort candidates by their true distances from the query\n",
    "    candidate_list = list(candidate_set)\n",
    "    candidates = X_tfidf[candidate_list]\n",
    "    distance = pairwise_distances(candidates, query_vector, metric='cosine').flatten()\n",
    "    \n",
    "    distance_col = 'distance'\n",
    "    nearest_neighbors = pd.DataFrame({\n",
    "        'Id': candidate_list, distance_col: distance\n",
    "    }).sort_values(distance_col).reset_index(drop=True)\n",
    "    return nearest_neighbors\n",
    "end_build_lsh_cos = time.time()\n",
    "build_time_lsh_cos = end_build_lsh_cos - start_build_lsh_cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_query_lsh_cos = time.time()\n",
    "\n",
    "similar_items = []\n",
    "counter = 0\n",
    "col = ['Train Id', 'Test Id']\n",
    "LSH_cos= DataFrame(columns = col)\n",
    "LSH_cos_dup = DataFrame(columns = ['#Duplicates','Parameters'])\n",
    "\n",
    "for rad in range(10):\n",
    "    for item_id in range(X_tfidf.shape[0]-test_head.shape[0]):\n",
    "        query_vector = X_tfidf[item_id]\n",
    "        nearest_neighbors = get_nearest_neighbors(X_tfidf, query_vector, model, max_search_radius=rad)\n",
    "        if nearest_neighbors['distance'][1]<0.165:\n",
    "            if nearest_neighbors['Id'][1]>train_head.shape[0]:\n",
    "                LSH_cos.at[counter, 'Train Id'] = item_id\n",
    "                LSH_cos.at[counter, 'Test Id'] = nearest_neighbors.iloc[1][0]\n",
    "                counter = counter + 1\n",
    "    LSH_cos_dup.at[rad, '#Duplicates'] = len(LSH_cos)\n",
    "    LSH_cos_dup.at[rad, '#Duplicates'] = rad\n",
    "print(LSH_cos_dup)\n",
    "\n",
    "end_query_lsh_cos = time.time()\n",
    "query_time_lsh_cos = end_query_lsh_cos - start_query_lsh_cos\n",
    "\n",
    "total_time_lsh_cos = build_time_lsh_cos + query_time_lsh_cos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSH Jaccard - Set number of permutations to {16,32,64}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_dataset['Content'].str.split()\n",
    "set_train = []\n",
    "for i in range(len(train)):\n",
    "    set_train.append(set(train[i])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test_dataset['Content'].str.split()\n",
    "set_test = []\n",
    "\n",
    "for i in range(len(test)):\n",
    "    set_test.append(set(test[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinHash Signatures for 16 permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSH Index Creation\n",
    "start_buildTime_16 = time.time()\n",
    "\n",
    "train_hashed_16 = list(range(0, train_dataset['Content'].shape[0]))\n",
    "lsh_16 = MinHashLSH(threshold = 0.8, num_perm = 16)\n",
    "\n",
    "for i in range(len(train_hashed_16)):\n",
    "    train_hashed_16[i] = MinHash(num_perm = 16)\n",
    "    for j in set_train[i]:\n",
    "        train_hashed_16[i].update(j.encode('utf8'))\n",
    "    lsh_16.insert(str(i), train_hashed_16[i])\n",
    "\n",
    "build_time_16 = time.time() - start_buildTime_16\n",
    "\n",
    "count_16 = 0\n",
    "start_queryTime_16 = time.time()\n",
    "\n",
    "test_hashed_16 = list(range(0, test_dataset['Content'].shape[0]))\n",
    "for i in range(len(test_hashed_16)):\n",
    "    test_hashed_16[i] = MinHash(num_perm = 16)\n",
    "    for j in set_test[i]:\n",
    "        test_hashed_16[i].update(j.encode('utf8'))\n",
    "\n",
    "for i in range(len(test_hashed_16)):\n",
    "    if len(lsh_16.query(test_hashed_16[i])):\n",
    "        count_16 += 1\n",
    "\n",
    "query_time_16 = time.time() - start_queryTime_16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build Time:  223.6151099205017\n",
      "Query Time:  2.1258325576782227\n",
      "Total Time:  225.74094247817993\n",
      "Duplicates:  617\n",
      "Parameters - Number of Permutations: 16\n"
     ]
    }
   ],
   "source": [
    "print(\"Build Time: \", build_time_16)\n",
    "print(\"Query Time: \", query_time_16)\n",
    "print(\"Total Time: \", build_time_16 + query_time_16)\n",
    "print(\"Duplicates: \", count_16)\n",
    "print(\"Parameters - Number of Permutations: 16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinHash Signatures for 32 permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSH Index Creation\n",
    "start_buildTime_32 = time.time()\n",
    "\n",
    "train_hashed_32 = list(range(0, train_dataset['Content'].shape[0]))\n",
    "lsh_32 = MinHashLSH(threshold = 0.8, num_perm = 32)\n",
    "\n",
    "for i in range(len(train_hashed_32)):\n",
    "    train_hashed_32[i] = MinHash(num_perm = 32)\n",
    "    for j in set_train[i]:\n",
    "        train_hashed_32[i].update(j.encode('utf8'))\n",
    "    lsh_32.insert(str(i), train_hashed_32[i])\n",
    "\n",
    "build_time_32 = time.time() - start_buildTime_32\n",
    "\n",
    "count_32 = 0\n",
    "start_queryTime_32 = time.time()\n",
    "\n",
    "test_hashed_32 = list(range(0, test_dataset['Content'].shape[0]))\n",
    "for i in range(len(test_hashed_32)):\n",
    "    test_hashed_32[i] = MinHash(num_perm = 32)\n",
    "    for j in set_test[i]:\n",
    "        test_hashed_32[i].update(j.encode('utf8'))\n",
    "\n",
    "for i in range(len(test_hashed_32)):\n",
    "    if len(lsh_32.query(test_hashed_32[i])):\n",
    "        count_32 += 1\n",
    "\n",
    "query_time_32 = time.time() - start_queryTime_32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build Time:  277.75269055366516\n",
      "Query Time:  2.739528179168701\n",
      "Total Time:  280.49221873283386\n",
      "Duplicates:  500\n",
      "Parameters - Number of Permutations: 32\n"
     ]
    }
   ],
   "source": [
    "print(\"Build Time: \", build_time_32)\n",
    "print(\"Query Time: \", query_time_32)\n",
    "print(\"Total Time: \", build_time_32 + query_time_32)\n",
    "print(\"Duplicates: \", count_32)\n",
    "print(\"Parameters - Number of Permutations: 32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinHash Signatures for 64 permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSH Index Creation\n",
    "start_buildTime_64 = time.time()\n",
    "\n",
    "train_hashed_64 = list(range(0, train_dataset['Content'].shape[0]))\n",
    "lsh_64 = MinHashLSH(threshold = 0.8, num_perm = 64)\n",
    "\n",
    "for i in range(len(train_hashed_64)):\n",
    "    train_hashed_64[i] = MinHash(num_perm = 64)\n",
    "    for j in set_train[i]:\n",
    "        train_hashed_64[i].update(j.encode('utf8'))\n",
    "    lsh_64.insert(str(i), train_hashed_64[i])\n",
    "\n",
    "build_time_64 = time.time() - start_buildTime_64\n",
    "\n",
    "count_64 = 0\n",
    "start_queryTime_64 = time.time()\n",
    "\n",
    "test_hashed_64 = list(range(0, test_dataset['Content'].shape[0]))\n",
    "for i in range(len(test_hashed_64)):\n",
    "    test_hashed_64[i] = MinHash(num_perm = 64)\n",
    "    for j in set_test[i]:\n",
    "        test_hashed_64[i].update(j.encode('utf8'))\n",
    "\n",
    "for i in range(len(test_hashed_64)):\n",
    "    if len(lsh_64.query(test_hashed_64[i])):\n",
    "        count_64 += 1\n",
    "\n",
    "query_time_64 = time.time() - start_queryTime_64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build Time:  422.53412795066833\n",
      "Query Time:  3.790764331817627\n",
      "Total Time:  426.32489228248596\n",
      "Duplicates:  541\n",
      "Parameters - Number of Permutations: 64\n"
     ]
    }
   ],
   "source": [
    "print(\"Build Time: \", build_time_64)\n",
    "print(\"Query Time: \", query_time_64)\n",
    "print(\"Total Time: \", build_time_64 + query_time_64)\n",
    "print(\"Duplicates: \", count_64)\n",
    "print(\"Parameters - Number of Permutations: 64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
